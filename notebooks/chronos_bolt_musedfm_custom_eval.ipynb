{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quick Start: Running Chronos Bolt on MUSED-FM Benchmark\n",
        "\n",
        "This notebook shows how to run Chronos Bolt models on the MUSED-FM benchmark using the `run_musedfm.py` script.\n",
        "\n",
        "Make sure you have the MUSED-FM benchmark data downloaded and set the `--benchmark-path` correctly before running this notebook.\n",
        "\n",
        "We will use the MUSED-FM framework to load the data and run the Chronos Bolt model. This notebook demonstrates how to integrate Chronos Bolt with the MUSED-FM evaluation framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Install required packages:\n",
        "\n",
        "```bash\n",
        "pip install chronos-forecasting\n",
        "pip install musedfm\n",
        "```\n",
        "\n",
        "Make sure you have PyTorch installed with CUDA support if you want to use GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MUSED-FM components imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Add the src directory to the Python path\n",
        "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
        "\n",
        "# Import MUSED-FM components\n",
        "from musedfm.data import Benchmark\n",
        "from musedfm.metrics import MAPE, MAE, RMSE, NMAE\n",
        "\n",
        "print(\"MUSED-FM components imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChronosForecast class defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Self-contained ChronosForecast class for the notebook\n",
        "class ChronosForecast:\n",
        "    \"\"\"\n",
        "    Chronos forecasting model wrapper for MUSED-FM evaluation.\n",
        "    This class is self-contained within the notebook.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_path: str = \"amazon/chronos-bolt-base\", device: str = \"cuda:0\", num_samples: int = 20):\n",
        "        \"\"\"\n",
        "        Initialize Chronos forecast model.\n",
        "        \n",
        "        Args:\n",
        "            model_path: Path to Chronos model (HuggingFace model ID or local path)\n",
        "            device: Device to run the model on\n",
        "            num_samples: Number of samples for probabilistic forecasting\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.device = device\n",
        "        self.num_samples = num_samples\n",
        "        self.pipeline = None\n",
        "        self._load_model()\n",
        "    \n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the Chronos model.\"\"\"\n",
        "        try:\n",
        "            from chronos import BaseChronosPipeline, ForecastType\n",
        "            \n",
        "            self.pipeline = BaseChronosPipeline.from_pretrained(\n",
        "                self.model_path,\n",
        "                device_map=self.device,\n",
        "            )\n",
        "            print(f\"Loaded Chronos model: {self.model_path}\")\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Chronos package not installed. Please install with: pip install chronos-forecasting\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load Chronos model: {e}\")\n",
        "    \n",
        "    def forecast(self, history: np.ndarray, covariates: Optional[np.ndarray] = None, forecast_horizon: Optional[int] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate forecast from historical data using Chronos.\n",
        "        \n",
        "        Args:\n",
        "            history: Historical time series data\n",
        "            covariates: Optional covariate data (ignored for Chronos)\n",
        "            forecast_horizon: Number of future points to forecast (default: 1)\n",
        "            \n",
        "        Returns:\n",
        "            Forecast values\n",
        "        \"\"\"\n",
        "        if forecast_horizon is None:\n",
        "            forecast_horizon = 1\n",
        "        \n",
        "        try:\n",
        "            # Convert history to torch tensor\n",
        "            if isinstance(history, np.ndarray):\n",
        "                history_tensor = torch.tensor(history, dtype=torch.float32)\n",
        "            else:\n",
        "                history_tensor = torch.tensor(np.array(history), dtype=torch.float32)\n",
        "            \n",
        "            # Remove NaN values\n",
        "            history_clean = history_tensor[~torch.isnan(history_tensor)]\n",
        "            \n",
        "            if len(history_clean) == 0:\n",
        "                # If no valid data, return zeros\n",
        "                return np.zeros(forecast_horizon)\n",
        "            \n",
        "            # Ensure we have enough history for forecasting\n",
        "            if len(history_clean) < 2:\n",
        "                # If insufficient data, return the last value repeated\n",
        "                last_value = float(history_clean[-1]) if len(history_clean) > 0 else 0.0\n",
        "                return np.full(forecast_horizon, last_value)\n",
        "            \n",
        "            # Generate forecast using Chronos\n",
        "            context = [history_clean]\n",
        "            \n",
        "            # Determine prediction kwargs based on forecast type\n",
        "            predict_kwargs = {}\n",
        "            if hasattr(self.pipeline, 'forecast_type'):\n",
        "                from chronos import ForecastType\n",
        "                if self.pipeline.forecast_type == ForecastType.SAMPLES:\n",
        "                    predict_kwargs = {\"num_samples\": self.num_samples}\n",
        "            \n",
        "            # Generate forecast\n",
        "            forecast_output = self.pipeline.predict(\n",
        "                context,\n",
        "                prediction_length=forecast_horizon,\n",
        "                **predict_kwargs\n",
        "            )\n",
        "            \n",
        "            # Convert to numpy array\n",
        "            if isinstance(forecast_output, torch.Tensor):\n",
        "                forecast_np = forecast_output.numpy()\n",
        "            else:\n",
        "                forecast_np = np.array(forecast_output)\n",
        "            \n",
        "            # Handle different output shapes\n",
        "            if forecast_np.ndim > 1:\n",
        "                # If we have multiple samples, take the mean\n",
        "                if forecast_np.shape[0] > 1:\n",
        "                    forecast_np = np.mean(forecast_np, axis=0)\n",
        "                else:\n",
        "                    forecast_np = forecast_np[0]\n",
        "            \n",
        "            # Ensure we have the right length\n",
        "            if len(forecast_np) != forecast_horizon:\n",
        "                if len(forecast_np) > forecast_horizon:\n",
        "                    forecast_np = forecast_np[:forecast_horizon]\n",
        "                else:\n",
        "                    # Pad with the last value if needed\n",
        "                    last_val = forecast_np[-1] if len(forecast_np) > 0 else 0.0\n",
        "                    forecast_np = np.pad(forecast_np, (0, forecast_horizon - len(forecast_np)), 'constant', constant_values=last_val)\n",
        "            \n",
        "            return forecast_np\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Chronos forecasting failed: {e}\")\n",
        "            # Fallback to simple mean forecast\n",
        "            if len(history) > 0:\n",
        "                mean_val = np.nanmean(history)\n",
        "                return np.full(forecast_horizon, mean_val)\n",
        "            else:\n",
        "                return np.zeros(forecast_horizon)\n",
        "\n",
        "print(\"ChronosForecast class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the benchmark path and model parameters. Adjust these according to your setup.\n",
        "ma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benchmark path: /workspace/data/fm_eval_nested/\n",
            "Model: amazon/chronos-bolt-base\n",
            "Device: cuda:0\n",
            "Output directory: ./results/chronos_bolt\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "BENCHMARK_PATH = \"/workspace/data/fm_eval_nested/\"  # Adjust this path to your MUSED-FM data\n",
        "MODEL_PATH = \"amazon/chronos-bolt-base\"  # Chronos Bolt model\n",
        "DEVICE = \"cuda:0\"  # Use \"cpu\" if you don't have CUDA\n",
        "NUM_SAMPLES = 20  # Number of samples for probabilistic forecasting\n",
        "MAX_WINDOWS = 50  # Limit windows per dataset for faster testing\n",
        "OUTPUT_DIR = \"./results/chronos_bolt\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Benchmark path: {BENCHMARK_PATH}\")\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Chronos Model\n",
        "\n",
        "Create a ChronosForecast instance that integrates with the MUSED-FM framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded Chronos model: amazon/chronos-bolt-base\n",
            "Chronos model initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize Chronos model\n",
        "try:\n",
        "    chronos_model = ChronosForecast(\n",
        "        model_path=MODEL_PATH,\n",
        "        device=DEVICE,\n",
        "        num_samples=NUM_SAMPLES\n",
        "    )\n",
        "    print(\"Chronos model initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Chronos model: {e}\")\n",
        "    print(\"Make sure you have installed chronos-forecasting and have the required dependencies.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Chronos Model Directly in Notebook\n",
        "\n",
        "Instead of using `run_musedfm.py`, we can run Chronos directly in the notebook for more control and immediate results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading MUSED-FM benchmark...\n",
            "Large dataset cifar100_timeseries_csvs detected (50000 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 27300000 total windows (546.0 avg per file)\n",
            "Loading KITTI data from /workspace/data/fm_eval_nested/sequential/KITTI\n",
            "Found 6114 parquet files\n",
            "Successfully loaded 6114 valid files\n",
            "Large dataset KITTI detected (6114 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 1016819 total windows (166.3 avg per file)\n",
            "Large dataset ant_csv_out detected (10134 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 5365243 total windows (529.4 avg per file)\n",
            "Large dataset hopper_csv_out detected (8470 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 1351981 total windows (159.6 avg per file)\n",
            "Large dataset spriteworld detected (19534 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 9321429 total windows (477.2 avg per file)\n",
            "Large dataset walker2d_csv_out detected (5476 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 1350107 total windows (246.6 avg per file)\n",
            "Large dataset cheetah_csv_out detected (5198 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 5042060 total windows (970.0 avg per file)\n",
            "Large dataset openwebtext_timeseries_csvs detected (100000 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 48200000 total windows (482.0 avg per file)\n",
            "Domain ALL_DATASETS not found in file hierarchy\n",
            "Large dataset large_convlag_synin_s detected (150429 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 111016602 total windows (738.0 avg per file)\n",
            "Large dataset medium_convlag_synin_s detected (64000 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 47232000 total windows (738.0 avg per file)\n",
            "Large dataset medium_obslag_synin_s detected (64000 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 47232000 total windows (738.0 avg per file)\n",
            "Large dataset tiny_convlag_synin_ns detected (64000 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 47232000 total windows (738.0 avg per file)\n",
            "Large dataset tiny_obslag_synin_ns detected (64000 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 47232000 total windows (738.0 avg per file)\n",
            "Large dataset dynamic_data_csvs detected (210232 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 208970608 total windows (994.0 avg per file)\n",
            "Domain ALL_DATASETS not found in file hierarchy\n",
            "Dataset aus_electricity not found in data_hierarchy.json\n",
            "/workspace/data/fm_eval_nested/traditional/open_aq [PosixPath('/workspace/data/fm_eval_nested/traditional/open_aq/delhi_combined.parquet'), PosixPath('/workspace/data/fm_eval_nested/traditional/open_aq/reykjavik_combined.parquet'), PosixPath('/workspace/data/fm_eval_nested/traditional/open_aq/rotterdam_combined.parquet'), PosixPath('/workspace/data/fm_eval_nested/traditional/open_aq/winnipeg_combined.parquet')]\n",
            "Domain ALL_DATASETS not found in file hierarchy\n",
            "Large dataset stock_nasdaqtrader detected (468 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 456768 total windows (976.0 avg per file)\n",
            "Large dataset wikipedia detected (1530 files). Estimating window count by sampling first 100 files...\n",
            "Estimated 95472 total windows (62.4 avg per file)\n",
            "Domain ALL_DATASETS not found in file hierarchy\n",
            "Benchmark loaded successfully!\n",
            "Number of categories: 4\n",
            "Category: sequential (4 domains)\n",
            "  Domain: Image (1 datasets)\n",
            "  Domain: Video (1 datasets)\n",
            "  Domain: Scientific (5 datasets)\n",
            "  Domain: Text (1 datasets)\n",
            "Category: synthetic (2 domains)\n",
            "  Domain: Causal Model (5 datasets)\n",
            "  Domain: Dynamic (1 datasets)\n",
            "Category: traditional (8 domains)\n",
            "  Domain: Energy (34 datasets)\n",
            "  Domain: Sales (6 datasets)\n",
            "  Domain: Environment (7 datasets)\n",
            "  Domain: Public Info (10 datasets)\n",
            "  Domain: Engineering (2 datasets)\n",
            "  Domain: Finance (8 datasets)\n",
            "  Domain: Health (2 datasets)\n",
            "  Domain: Web (1 datasets)\n",
            "Category: collections (2 domains)\n",
            "  Domain: Stock (1 datasets)\n",
            "  Domain: Wikipedia (1 datasets)\n",
            "Total datasets in benchmark: 86\n"
          ]
        }
      ],
      "source": [
        "# Load the MUSED-FM benchmark\n",
        "print(\"Loading MUSED-FM benchmark...\")\n",
        "try:\n",
        "    benchmark = Benchmark(BENCHMARK_PATH)\n",
        "    print(f\"Benchmark loaded successfully!\")\n",
        "    print(f\"Number of categories: {len(benchmark)}\")\n",
        "    \n",
        "    # Print some basic info about the benchmark\n",
        "    total_datasets = 0\n",
        "    for category in benchmark:\n",
        "        print(f\"Category: {category.category} ({len(category)} domains)\")\n",
        "        for domain in category:\n",
        "            print(f\"  Domain: {domain.domain_name} ({len(domain)} datasets)\")\n",
        "            total_datasets += len(domain)\n",
        "    \n",
        "    print(f\"Total datasets in benchmark: {total_datasets}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading benchmark: {e}\")\n",
        "    print(f\"Make sure the benchmark path '{BENCHMARK_PATH}' is correct and contains MUSED-FM data.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Direct Evaluation\n",
        "\n",
        "Now let's run Chronos directly on the benchmark data for immediate results and full control.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Chronos evaluation...\n",
            "Starting direct evaluation on up to 3 datasets...\n",
            "\n",
            "Evaluating dataset: cifar100_timeseries_csvs (1/3)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Domain' object has no attribute 'domain'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Run direct evaluation\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Chronos evaluation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m direct_results = \u001b[43mevaluate_chronos_directly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchronos_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_datasets\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_windows_per_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDirect evaluation completed on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(direct_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m datasets\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mevaluate_chronos_directly\u001b[39m\u001b[34m(benchmark, model, max_datasets, max_windows_per_dataset)\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset.dataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_count\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_datasets\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCategory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory.category\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Domain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdomain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdomain\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m windows\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Limit windows for faster evaluation\u001b[39;00m\n",
            "\u001b[31mAttributeError\u001b[39m: 'Domain' object has no attribute 'domain'"
          ]
        }
      ],
      "source": [
        "# Direct evaluation function\n",
        "def evaluate_chronos_directly(benchmark, model, max_datasets=5, max_windows_per_dataset=10):\n",
        "    \"\"\"\n",
        "    Directly evaluate Chronos model on benchmark data.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    dataset_count = 0\n",
        "    \n",
        "    print(f\"Starting direct evaluation on up to {max_datasets} datasets...\")\n",
        "    \n",
        "    for category in benchmark:\n",
        "        if dataset_count >= max_datasets:\n",
        "            break\n",
        "            \n",
        "        for domain in category:\n",
        "            if dataset_count >= max_datasets:\n",
        "                break\n",
        "                \n",
        "            for dataset in domain:\n",
        "                if dataset_count >= max_datasets:\n",
        "                    break\n",
        "                    \n",
        "                print(f\"\\nEvaluating dataset: {dataset.dataset_name} ({dataset_count + 1}/{max_datasets})\")\n",
        "                print(f\"Category: {category.category}, Domain: {domain.domain}\")\n",
        "                print(f\"Dataset size: {len(dataset)} windows\")\n",
        "                \n",
        "                # Limit windows for faster evaluation\n",
        "                windows_processed = 0\n",
        "                dataset_metrics = {'MAPE': [], 'MAE': [], 'RMSE': [], 'NMAE': []}\n",
        "                \n",
        "                for window in dataset:\n",
        "                    if windows_processed >= max_windows_per_dataset:\n",
        "                        break\n",
        "                        \n",
        "                    try:\n",
        "                        # Get history and future data\n",
        "                        history = window.history\n",
        "                        future = window.future\n",
        "                        forecast_horizon = len(future)\n",
        "                        \n",
        "                        # Generate forecast\n",
        "                        forecast = model.forecast(\n",
        "                            history=history,\n",
        "                            forecast_horizon=forecast_horizon\n",
        "                        )\n",
        "                        \n",
        "                        # Calculate metrics\n",
        "                        mape = MAPE(future, forecast)\n",
        "                        mae = MAE(future, forecast)\n",
        "                        rmse = RMSE(future, forecast)\n",
        "                        nmae = NMAE(future, forecast)\n",
        "                        \n",
        "                        dataset_metrics['MAPE'].append(mape)\n",
        "                        dataset_metrics['MAE'].append(mae)\n",
        "                        dataset_metrics['RMSE'].append(rmse)\n",
        "                        dataset_metrics['NMAE'].append(nmae)\n",
        "                        \n",
        "                        windows_processed += 1\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Error processing window {windows_processed}: {e}\")\n",
        "                        continue\n",
        "                \n",
        "                # Calculate average metrics for this dataset\n",
        "                if windows_processed > 0:\n",
        "                    avg_metrics = {}\n",
        "                    for metric_name, values in dataset_metrics.items():\n",
        "                        if values:\n",
        "                            avg_metrics[metric_name] = np.mean(values)\n",
        "                        else:\n",
        "                            avg_metrics[metric_name] = np.nan\n",
        "                    \n",
        "                    result = {\n",
        "                        'dataset': dataset.dataset_name,\n",
        "                        'category': category.category,\n",
        "                        'domain': domain.domain,\n",
        "                        'windows_processed': windows_processed,\n",
        "                        **avg_metrics\n",
        "                    }\n",
        "                    results.append(result)\n",
        "                    \n",
        "                    print(f\"Processed {windows_processed} windows\")\n",
        "                    print(f\"Average MAPE: {avg_metrics['MAPE']:.4f}\")\n",
        "                    print(f\"Average MAE: {avg_metrics['MAE']:.4f}\")\n",
        "                    print(f\"Average RMSE: {avg_metrics['RMSE']:.4f}\")\n",
        "                    print(f\"Average NMAE: {avg_metrics['NMAE']:.4f}\")\n",
        "                \n",
        "                dataset_count += 1\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run direct evaluation\n",
        "print(\"Starting Chronos evaluation...\")\n",
        "direct_results = evaluate_chronos_directly(benchmark, chronos_model, max_datasets=3, max_windows_per_dataset=5)\n",
        "print(f\"\\nDirect evaluation completed on {len(direct_results)} datasets\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import plotting utilities\n",
        "import matplotlib.pyplot as plt\n",
        "from musedfm.plotting import plot_window_forecasts, plot_multiple_windows, plot_baseline_comparison\n",
        "\n",
        "# Plot forecasts for the first few windows from each dataset\n",
        "def plot_chronos_forecasts(benchmark, model, max_datasets=3, max_windows_per_dataset=3):\n",
        "    \"\"\"\n",
        "    Plot Chronos forecasts for visualization.\n",
        "    \"\"\"\n",
        "    plot_data = []\n",
        "    dataset_count = 0\n",
        "    \n",
        "    print(f\"Collecting plot data from up to {max_datasets} datasets...\")\n",
        "    \n",
        "    for category in benchmark:\n",
        "        if dataset_count >= max_datasets:\n",
        "            break\n",
        "            \n",
        "        for domain in category:\n",
        "            if dataset_count >= max_datasets:\n",
        "                break\n",
        "                \n",
        "            for dataset in domain:\n",
        "                if dataset_count >= max_datasets:\n",
        "                    break\n",
        "                    \n",
        "                print(f\"\\nCollecting plot data from: {dataset.dataset_name}\")\n",
        "                \n",
        "                # Collect windows and forecasts for plotting\n",
        "                windows_processed = 0\n",
        "                dataset_forecasts = {}\n",
        "                \n",
        "                for window in dataset:\n",
        "                    if windows_processed >= max_windows_per_dataset:\n",
        "                        break\n",
        "                        \n",
        "                    try:\n",
        "                        # Get history and future data\n",
        "                        history = window.history\n",
        "                        future = window.future\n",
        "                        forecast_horizon = len(future)\n",
        "                        \n",
        "                        # Generate forecast\n",
        "                        forecast = model.forecast(\n",
        "                            history=history,\n",
        "                            forecast_horizon=forecast_horizon\n",
        "                        )\n",
        "                        \n",
        "                        # Store window and forecast data\n",
        "                        plot_data.append({\n",
        "                            'window': window,\n",
        "                            'forecast': forecast,\n",
        "                            'dataset_name': dataset.dataset_name,\n",
        "                            'category': category.category,\n",
        "                            'domain': domain.domain_name,\n",
        "                            'window_idx': windows_processed\n",
        "                        })\n",
        "                        \n",
        "                        windows_processed += 1\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Error processing window {windows_processed}: {e}\")\n",
        "                        continue\n",
        "                \n",
        "                print(f\"Collected {windows_processed} windows for plotting\")\n",
        "                dataset_count += 1\n",
        "    \n",
        "    return plot_data\n",
        "\n",
        "# Collect plot data\n",
        "plot_data = plot_chronos_forecasts(benchmark, chronos_model, max_datasets=3, max_windows_per_dataset=3)\n",
        "\n",
        "print(f\"\\nCollected plot data for {len(plot_data)} windows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create individual plots for each window\n",
        "if plot_data:\n",
        "    print(\"Creating individual forecast plots...\")\n",
        "    \n",
        "    # Create plots directory\n",
        "    plots_dir = os.path.join(OUTPUT_DIR, \"plots\")\n",
        "    os.makedirs(plots_dir, exist_ok=True)\n",
        "    \n",
        "    for i, data in enumerate(plot_data[:9]):  # Plot first 9 windows\n",
        "        window = data['window']\n",
        "        forecast = data['forecast']\n",
        "        dataset_name = data['dataset_name']\n",
        "        category = data['category']\n",
        "        domain = data['domain']\n",
        "        window_idx = data['window_idx']\n",
        "        \n",
        "        # Create forecasts dictionary for the plotting function\n",
        "        forecasts = {'Chronos Bolt': forecast}\n",
        "        \n",
        "        # Create title\n",
        "        title = f\"{dataset_name} - Window {window_idx}\\nCategory: {category}, Domain: {domain}\"\n",
        "        \n",
        "        # Plot the window\n",
        "        plot_window_forecasts(\n",
        "            window=window,\n",
        "            forecasts=forecasts,\n",
        "            title=title,\n",
        "            figsize=(12, 6),\n",
        "            save_path=os.path.join(plots_dir, f\"chronos_forecast_{i+1}.png\")\n",
        "        )\n",
        "        \n",
        "        print(f\"Saved plot {i+1}: {dataset_name} - Window {window_idx}\")\n",
        "    \n",
        "    print(f\"\\nIndividual plots saved to: {plots_dir}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No plot data available.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a multi-window comparison plot\n",
        "if plot_data and len(plot_data) >= 3:\n",
        "    print(\"Creating multi-window comparison plot...\")\n",
        "    \n",
        "    # Select first 6 windows for comparison\n",
        "    selected_data = plot_data[:6]\n",
        "    windows = [data['window'] for data in selected_data]\n",
        "    forecasts_dict = {'Chronos Bolt': {i: data['forecast'] for i, data in enumerate(selected_data)}}\n",
        "    \n",
        "    # Create window titles\n",
        "    window_titles = []\n",
        "    for data in selected_data:\n",
        "        title = f\"{data['dataset_name']}\\nWindow {data['window_idx']}\"\n",
        "        window_titles.append(title)\n",
        "    \n",
        "    # Create multi-window plot\n",
        "    plot_multiple_windows(\n",
        "        windows=windows,\n",
        "        forecasts_dict=forecasts_dict,\n",
        "        window_titles=window_titles,\n",
        "        figsize=(18, 12),\n",
        "        save_path=os.path.join(plots_dir, \"chronos_multi_window_comparison.png\")\n",
        "    )\n",
        "    \n",
        "    print(\"Multi-window comparison plot saved!\")\n",
        "    \n",
        "    # Also create a baseline comparison plot for the first window\n",
        "    if plot_data:\n",
        "        first_data = plot_data[0]\n",
        "        window = first_data['window']\n",
        "        forecast = first_data['forecast']\n",
        "        \n",
        "        # Calculate metrics for the baseline comparison\n",
        "        future = window.future\n",
        "        metrics = {\n",
        "            'Chronos Bolt': {\n",
        "                'MAPE': MAPE(future, forecast),\n",
        "                'MAE': MAE(future, forecast),\n",
        "                'RMSE': RMSE(future, forecast),\n",
        "                'NMAE': NMAE(future, forecast)\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        forecasts = {'Chronos Bolt': forecast}\n",
        "        title = f\"Chronos Bolt Performance\\n{first_data['dataset_name']} - Window {first_data['window_idx']}\"\n",
        "        \n",
        "        plot_baseline_comparison(\n",
        "            window=window,\n",
        "            forecasts=forecasts,\n",
        "            metrics=metrics,\n",
        "            title=title,\n",
        "            figsize=(15, 8),\n",
        "            save_path=os.path.join(plots_dir, \"chronos_baseline_comparison.png\")\n",
        "        )\n",
        "        \n",
        "        print(\"Baseline comparison plot saved!\")\n",
        "    \n",
        "    print(f\"\\nAll plots saved to: {plots_dir}\")\n",
        "    print(\"You can view the plots to see how Chronos Bolt performs on different datasets!\")\n",
        "    \n",
        "else:\n",
        "    print(\"Not enough data for multi-window comparison plot.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results\n",
        "if direct_results:\n",
        "    df_results = pd.DataFrame(direct_results)\n",
        "    print(\"Evaluation Results:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(df_results.to_string(index=False))\n",
        "    \n",
        "    # Calculate overall averages\n",
        "    print(\"\\nOverall Average Metrics:\")\n",
        "    print(\"=\" * 30)\n",
        "    numeric_cols = ['MAPE', 'MAE', 'RMSE', 'NMAE']\n",
        "    for col in numeric_cols:\n",
        "        if col in df_results.columns:\n",
        "            avg_val = df_results[col].mean()\n",
        "            print(f\"{col}: {avg_val:.6f}\")\n",
        "    \n",
        "    # Save results to CSV\n",
        "    results_file = os.path.join(OUTPUT_DIR, \"chronos_results.csv\")\n",
        "    df_results.to_csv(results_file, index=False)\n",
        "    print(f\"\\nResults saved to: {results_file}\")\n",
        "    \n",
        "    # Save summary\n",
        "    summary_file = os.path.join(OUTPUT_DIR, \"summary.txt\")\n",
        "    with open(summary_file, 'w') as f:\n",
        "        f.write(\"Chronos Bolt Evaluation Summary\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Total datasets evaluated: {len(direct_results)}\\n\")\n",
        "        f.write(f\"Model: {MODEL_PATH}\\n\")\n",
        "        f.write(f\"Device: {DEVICE}\\n\")\n",
        "        f.write(f\"Max windows per dataset: 5\\n\\n\")\n",
        "        f.write(\"Overall Average Metrics:\\n\")\n",
        "        for col in numeric_cols:\n",
        "            if col in df_results.columns:\n",
        "                avg_val = df_results[col].mean()\n",
        "                f.write(f\"  {col}: {avg_val:.6f}\\n\")\n",
        "    \n",
        "    print(f\"Summary saved to: {summary_file}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No results available. Make sure the evaluation completed successfully.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
