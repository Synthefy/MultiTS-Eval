{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quick Start: Running Chronos Bolt on MUSED-FM Benchmark\n",
        "\n",
        "This notebook shows how to run Chronos Bolt models on the MUSED-FM benchmark using the `run_musedfm.py` script.\n",
        "\n",
        "Make sure you have the MUSED-FM benchmark data downloaded and set the `--benchmark-path` correctly before running this notebook.\n",
        "\n",
        "We will use the MUSED-FM framework to load the data and run the Chronos Bolt model. This notebook demonstrates how to integrate Chronos Bolt with the MUSED-FM evaluation framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Install required packages:\n",
        "\n",
        "```bash\n",
        "pip install chronos-forecasting\n",
        "pip install musedfm\n",
        "```\n",
        "\n",
        "Make sure you have PyTorch installed with CUDA support if you want to use GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Add the src directory to the Python path\n",
        "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
        "\n",
        "# Import MUSED-FM components\n",
        "from musedfm.data import Benchmark\n",
        "from musedfm.metrics import MAPE, MAE, RMSE, NMAE\n",
        "\n",
        "print(\"MUSED-FM components imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Self-contained ChronosForecast class for the notebook\n",
        "class ChronosForecast:\n",
        "    \"\"\"\n",
        "    Chronos forecasting model wrapper for MUSED-FM evaluation.\n",
        "    This class is self-contained within the notebook.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_path: str = \"amazon/chronos-bolt-base\", device: str = \"cuda:0\", num_samples: int = 20):\n",
        "        \"\"\"\n",
        "        Initialize Chronos forecast model.\n",
        "        \n",
        "        Args:\n",
        "            model_path: Path to Chronos model (HuggingFace model ID or local path)\n",
        "            device: Device to run the model on\n",
        "            num_samples: Number of samples for probabilistic forecasting\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.device = device\n",
        "        self.num_samples = num_samples\n",
        "        self.pipeline = None\n",
        "        self._load_model()\n",
        "    \n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the Chronos model.\"\"\"\n",
        "        try:\n",
        "            from chronos import BaseChronosPipeline, ForecastType\n",
        "            \n",
        "            self.pipeline = BaseChronosPipeline.from_pretrained(\n",
        "                self.model_path,\n",
        "                device_map=self.device,\n",
        "            )\n",
        "            print(f\"Loaded Chronos model: {self.model_path}\")\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Chronos package not installed. Please install with: pip install chronos-forecasting\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load Chronos model: {e}\")\n",
        "    \n",
        "    def forecast(self, history: np.ndarray, covariates: Optional[np.ndarray] = None, forecast_horizon: Optional[int] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate forecast from historical data using Chronos.\n",
        "        \n",
        "        Args:\n",
        "            history: Historical time series data\n",
        "            covariates: Optional covariate data (ignored for Chronos)\n",
        "            forecast_horizon: Number of future points to forecast (default: 1)\n",
        "            \n",
        "        Returns:\n",
        "            Forecast values\n",
        "        \"\"\"\n",
        "        if forecast_horizon is None:\n",
        "            forecast_horizon = 1\n",
        "        \n",
        "        try:\n",
        "            # Convert history to torch tensor\n",
        "            if isinstance(history, np.ndarray):\n",
        "                history_tensor = torch.tensor(history, dtype=torch.float32)\n",
        "            else:\n",
        "                history_tensor = torch.tensor(np.array(history), dtype=torch.float32)\n",
        "            \n",
        "            # Remove NaN values\n",
        "            history_clean = history_tensor[~torch.isnan(history_tensor)]\n",
        "            \n",
        "            if len(history_clean) == 0:\n",
        "                # If no valid data, return zeros\n",
        "                return np.zeros(forecast_horizon)\n",
        "            \n",
        "            # Ensure we have enough history for forecasting\n",
        "            if len(history_clean) < 2:\n",
        "                # If insufficient data, return the last value repeated\n",
        "                last_value = float(history_clean[-1]) if len(history_clean) > 0 else 0.0\n",
        "                return np.full(forecast_horizon, last_value)\n",
        "            \n",
        "            # Generate forecast using Chronos\n",
        "            context = [history_clean]\n",
        "            \n",
        "            # Determine prediction kwargs based on forecast type\n",
        "            predict_kwargs = {}\n",
        "            if hasattr(self.pipeline, 'forecast_type'):\n",
        "                from chronos import ForecastType\n",
        "                if self.pipeline.forecast_type == ForecastType.SAMPLES:\n",
        "                    predict_kwargs = {\"num_samples\": self.num_samples}\n",
        "            \n",
        "            # Generate forecast\n",
        "            forecast_output = self.pipeline.predict(\n",
        "                context,\n",
        "                prediction_length=forecast_horizon,\n",
        "                **predict_kwargs\n",
        "            )\n",
        "            \n",
        "            # Convert to numpy array\n",
        "            if isinstance(forecast_output, torch.Tensor):\n",
        "                forecast_np = forecast_output.numpy()\n",
        "            else:\n",
        "                forecast_np = np.array(forecast_output)\n",
        "            \n",
        "            # Handle different output shapes\n",
        "            if forecast_np.ndim > 1:\n",
        "                # If we have multiple samples, take the mean\n",
        "                if forecast_np.shape[0] > 1:\n",
        "                    forecast_np = np.mean(forecast_np, axis=0)\n",
        "                else:\n",
        "                    forecast_np = forecast_np[0]\n",
        "            \n",
        "            # Ensure we have the right length\n",
        "            if len(forecast_np) != forecast_horizon:\n",
        "                if len(forecast_np) > forecast_horizon:\n",
        "                    forecast_np = forecast_np[:forecast_horizon]\n",
        "                else:\n",
        "                    # Pad with the last value if needed\n",
        "                    last_val = forecast_np[-1] if len(forecast_np) > 0 else 0.0\n",
        "                    forecast_np = np.pad(forecast_np, (0, forecast_horizon - len(forecast_np)), 'constant', constant_values=last_val)\n",
        "            \n",
        "            return forecast_np\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Chronos forecasting failed: {e}\")\n",
        "            # Fallback to simple mean forecast\n",
        "            if len(history) > 0:\n",
        "                mean_val = np.nanmean(history)\n",
        "                return np.full(forecast_horizon, mean_val)\n",
        "            else:\n",
        "                return np.zeros(forecast_horizon)\n",
        "\n",
        "print(\"ChronosForecast class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the benchmark path and model parameters. Adjust these according to your setup.\n",
        "ma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "BENCHMARK_PATH = \"/home/caleb/musedfm_data\"  # Adjust this path to your MUSED-FM data\n",
        "MODEL_PATH = \"amazon/chronos-bolt-base\"  # Chronos Bolt model\n",
        "DEVICE = \"cuda:0\"  # Use \"cpu\" if you don't have CUDA\n",
        "NUM_SAMPLES = 20  # Number of samples for probabilistic forecasting\n",
        "MAX_WINDOWS = 50  # Limit windows per dataset for faster testing\n",
        "OUTPUT_DIR = \"./results/chronos_bolt\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Benchmark path: {BENCHMARK_PATH}\")\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Chronos Model\n",
        "\n",
        "Create a ChronosForecast instance that integrates with the MUSED-FM framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Chronos model\n",
        "try:\n",
        "    chronos_model = ChronosForecast(\n",
        "        model_path=MODEL_PATH,\n",
        "        device=DEVICE,\n",
        "        num_samples=NUM_SAMPLES\n",
        "    )\n",
        "    print(\"Chronos model initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Chronos model: {e}\")\n",
        "    print(\"Make sure you have installed chronos-forecasting and have the required dependencies.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Chronos Model Directly in Notebook\n",
        "\n",
        "Instead of using `run_musedfm.py`, we can run Chronos directly in the notebook for more control and immediate results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Direct Evaluation\n",
        "\n",
        "Now let's run Chronos directly on the benchmark data for immediate results and full control.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Direct evaluation function\n",
        "def evaluate_chronos_directly(benchmark, model, max_datasets=5, max_windows_per_dataset=10):\n",
        "    \"\"\"\n",
        "    Directly evaluate Chronos model on benchmark data.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    dataset_count = 0\n",
        "    \n",
        "    print(f\"Starting direct evaluation on up to {max_datasets} datasets...\")\n",
        "    \n",
        "    for category in benchmark:\n",
        "        if dataset_count >= max_datasets:\n",
        "            break\n",
        "            \n",
        "        for domain in category:\n",
        "            if dataset_count >= max_datasets:\n",
        "                break\n",
        "                \n",
        "            for dataset in domain:\n",
        "                if dataset_count >= max_datasets:\n",
        "                    break\n",
        "                    \n",
        "                print(f\"\\nEvaluating dataset: {dataset.dataset_name} ({dataset_count + 1}/{max_datasets})\")\n",
        "                print(f\"Category: {category.category}, Domain: {domain.domain}\")\n",
        "                print(f\"Dataset size: {len(dataset)} windows\")\n",
        "                \n",
        "                # Limit windows for faster evaluation\n",
        "                windows_processed = 0\n",
        "                dataset_metrics = {'MAPE': [], 'MAE': [], 'RMSE': [], 'NMAE': []}\n",
        "                \n",
        "                for window in dataset:\n",
        "                    if windows_processed >= max_windows_per_dataset:\n",
        "                        break\n",
        "                        \n",
        "                    try:\n",
        "                        # Get history and future data\n",
        "                        history = window.history\n",
        "                        future = window.future\n",
        "                        forecast_horizon = len(future)\n",
        "                        \n",
        "                        # Generate forecast\n",
        "                        forecast = model.forecast(\n",
        "                            history=history,\n",
        "                            forecast_horizon=forecast_horizon\n",
        "                        )\n",
        "                        \n",
        "                        # Calculate metrics\n",
        "                        mape = MAPE(future, forecast)\n",
        "                        mae = MAE(future, forecast)\n",
        "                        rmse = RMSE(future, forecast)\n",
        "                        nmae = NMAE(future, forecast)\n",
        "                        \n",
        "                        dataset_metrics['MAPE'].append(mape)\n",
        "                        dataset_metrics['MAE'].append(mae)\n",
        "                        dataset_metrics['RMSE'].append(rmse)\n",
        "                        dataset_metrics['NMAE'].append(nmae)\n",
        "                        \n",
        "                        windows_processed += 1\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Error processing window {windows_processed}: {e}\")\n",
        "                        continue\n",
        "                \n",
        "                # Calculate average metrics for this dataset\n",
        "                if windows_processed > 0:\n",
        "                    avg_metrics = {}\n",
        "                    for metric_name, values in dataset_metrics.items():\n",
        "                        if values:\n",
        "                            avg_metrics[metric_name] = np.mean(values)\n",
        "                        else:\n",
        "                            avg_metrics[metric_name] = np.nan\n",
        "                    \n",
        "                    result = {\n",
        "                        'dataset': dataset.dataset_name,\n",
        "                        'category': category.category,\n",
        "                        'domain': domain.domain,\n",
        "                        'windows_processed': windows_processed,\n",
        "                        **avg_metrics\n",
        "                    }\n",
        "                    results.append(result)\n",
        "                    \n",
        "                    print(f\"Processed {windows_processed} windows\")\n",
        "                    print(f\"Average MAPE: {avg_metrics['MAPE']:.4f}\")\n",
        "                    print(f\"Average MAE: {avg_metrics['MAE']:.4f}\")\n",
        "                    print(f\"Average RMSE: {avg_metrics['RMSE']:.4f}\")\n",
        "                    print(f\"Average NMAE: {avg_metrics['NMAE']:.4f}\")\n",
        "                \n",
        "                dataset_count += 1\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run direct evaluation\n",
        "print(\"Starting Chronos evaluation...\")\n",
        "direct_results = evaluate_chronos_directly(benchmark, chronos_model, max_datasets=3, max_windows_per_dataset=5)\n",
        "print(f\"\\nDirect evaluation completed on {len(direct_results)} datasets\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results\n",
        "if direct_results:\n",
        "    df_results = pd.DataFrame(direct_results)\n",
        "    print(\"Evaluation Results:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(df_results.to_string(index=False))\n",
        "    \n",
        "    # Calculate overall averages\n",
        "    print(\"\\nOverall Average Metrics:\")\n",
        "    print(\"=\" * 30)\n",
        "    numeric_cols = ['MAPE', 'MAE', 'RMSE', 'NMAE']\n",
        "    for col in numeric_cols:\n",
        "        if col in df_results.columns:\n",
        "            avg_val = df_results[col].mean()\n",
        "            print(f\"{col}: {avg_val:.6f}\")\n",
        "    \n",
        "    # Save results to CSV\n",
        "    results_file = os.path.join(OUTPUT_DIR, \"chronos_results.csv\")\n",
        "    df_results.to_csv(results_file, index=False)\n",
        "    print(f\"\\nResults saved to: {results_file}\")\n",
        "    \n",
        "    # Save summary\n",
        "    summary_file = os.path.join(OUTPUT_DIR, \"summary.txt\")\n",
        "    with open(summary_file, 'w') as f:\n",
        "        f.write(\"Chronos Bolt Evaluation Summary\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Total datasets evaluated: {len(direct_results)}\\n\")\n",
        "        f.write(f\"Model: {MODEL_PATH}\\n\")\n",
        "        f.write(f\"Device: {DEVICE}\\n\")\n",
        "        f.write(f\"Max windows per dataset: 5\\n\\n\")\n",
        "        f.write(\"Overall Average Metrics:\\n\")\n",
        "        for col in numeric_cols:\n",
        "            if col in df_results.columns:\n",
        "                avg_val = df_results[col].mean()\n",
        "                f.write(f\"  {col}: {avg_val:.6f}\\n\")\n",
        "    \n",
        "    print(f\"Summary saved to: {summary_file}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No results available. Make sure the evaluation completed successfully.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
