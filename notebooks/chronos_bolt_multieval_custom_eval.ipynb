{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quick Start: Running Chronos Bolt on MultiTS-Eval Benchmark\n",
        "\n",
        "This notebook shows how to run Chronos Bolt models on the MultiTS-Eval benchmark using the `run_multieval.py` script.\n",
        "\n",
        "Make sure you have the MultiTS-Eval benchmark data downloaded and set the `--benchmark-path` correctly before running this notebook.\n",
        "\n",
        "We will use the MultiTS-Eval framework to load the data and run the Chronos Bolt model. This notebook demonstrates how to integrate Chronos Bolt with the MultiTS-Eval evaluation framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Install required packages:\n",
        "\n",
        "```bash\n",
        "pip install chronos-forecasting\n",
        "pip install multieval\n",
        "```\n",
        "\n",
        "Make sure you have PyTorch installed with CUDA support if you want to use GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultiTS-Eval components imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Add the src directory to the Python path\n",
        "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
        "\n",
        "# Import MultiTS-Eval components\n",
        "from multieval.data import Benchmark\n",
        "from multieval.metrics import MAPE, MAE, RMSE, NMAE\n",
        "\n",
        "print(\"MultiTS-Eval components imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChronosForecast class defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Self-contained ChronosForecast class for the notebook\n",
        "class ChronosForecast:\n",
        "    \"\"\"\n",
        "    Chronos forecasting model wrapper for MultiTS-Eval evaluation.\n",
        "    This class is self-contained within the notebook.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_path: str = \"amazon/chronos-bolt-base\", device: str = \"cuda:0\", num_samples: int = 20):\n",
        "        \"\"\"\n",
        "        Initialize Chronos forecast model.\n",
        "        \n",
        "        Args:\n",
        "            model_path: Path to Chronos model (HuggingFace model ID or local path)\n",
        "            device: Device to run the model on\n",
        "            num_samples: Number of samples for probabilistic forecasting\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.device = device\n",
        "        self.num_samples = num_samples\n",
        "        self.pipeline = None\n",
        "        self._load_model()\n",
        "    \n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the Chronos model.\"\"\"\n",
        "        try:\n",
        "            from chronos import BaseChronosPipeline, ForecastType\n",
        "            \n",
        "            self.pipeline = BaseChronosPipeline.from_pretrained(\n",
        "                self.model_path,\n",
        "                device_map=self.device,\n",
        "            )\n",
        "            print(f\"Loaded Chronos model: {self.model_path}\")\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Chronos package not installed. Please install with: pip install chronos-forecasting\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load Chronos model: {e}\")\n",
        "    \n",
        "    def forecast(self, history: np.ndarray, covariates: Optional[np.ndarray] = None, forecast_horizon: Optional[int] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate forecast from historical data using Chronos.\n",
        "        \n",
        "        Args:\n",
        "            history: Historical time series data\n",
        "            covariates: Optional covariate data (ignored for Chronos)\n",
        "            forecast_horizon: Number of future points to forecast (default: 1)\n",
        "            \n",
        "        Returns:\n",
        "            Forecast values\n",
        "        \"\"\"\n",
        "        if forecast_horizon is None:\n",
        "            forecast_horizon = 1\n",
        "        \n",
        "        # Convert history to torch tensor\n",
        "        if isinstance(history, np.ndarray):\n",
        "            history_tensor = torch.tensor(history, dtype=torch.float32)\n",
        "        else:\n",
        "            history_tensor = torch.tensor(np.array(history), dtype=torch.float32)\n",
        "        \n",
        "        # Remove NaN values\n",
        "        history_clean = history_tensor[~torch.isnan(history_tensor)]\n",
        "        \n",
        "        if len(history_clean) == 0:\n",
        "            # If no valid data, return zeros\n",
        "            return np.zeros(forecast_horizon)\n",
        "        \n",
        "        # Ensure we have enough history for forecasting\n",
        "        if len(history_clean) < 2:\n",
        "            # If insufficient data, return the last value repeated\n",
        "            last_value = float(history_clean[-1]) if len(history_clean) > 0 else 0.0\n",
        "            return np.full(forecast_horizon, last_value)\n",
        "        \n",
        "        # Generate forecast using Chronos\n",
        "        context = [history_clean]\n",
        "        \n",
        "        # Determine prediction kwargs based on forecast type\n",
        "        predict_kwargs = {}\n",
        "        if hasattr(self.pipeline, 'forecast_type'):\n",
        "            from chronos import ForecastType\n",
        "            if self.pipeline.forecast_type == ForecastType.SAMPLES:\n",
        "                predict_kwargs = {\"num_samples\": self.num_samples}\n",
        "        \n",
        "        # Generate forecast\n",
        "        forecast_output = self.pipeline.predict(\n",
        "            context,\n",
        "            prediction_length=forecast_horizon,\n",
        "            **predict_kwargs\n",
        "        )\n",
        "        \n",
        "        # Convert to numpy array\n",
        "        if isinstance(forecast_output, torch.Tensor):\n",
        "            forecast_np = forecast_output.numpy()\n",
        "        else:\n",
        "            forecast_np = np.array(forecast_output)\n",
        "        \n",
        "        # Handle different output shapes\n",
        "        if forecast_np.ndim > 1:\n",
        "            # Chronos Bolt returns (batch_size, num_quantiles, prediction_length)\n",
        "            # We want the median (0.5 quantile) which is typically at index 4 (0.1, 0.2, ..., 0.9)\n",
        "            if forecast_np.ndim == 3 and forecast_np.shape[1] == 9:  # Standard Chronos Bolt quantiles\n",
        "                # Take the median (0.5 quantile) at index 4\n",
        "                forecast_np = forecast_np[0, 4, :]  # (batch_size=1, quantile=4, prediction_length)\n",
        "            elif forecast_np.shape[0] > 1:\n",
        "                # If we have multiple samples, take the mean\n",
        "                forecast_np = np.mean(forecast_np, axis=0)\n",
        "            else:\n",
        "                forecast_np = forecast_np[0]\n",
        "        \n",
        "        # Ensure we have the right length\n",
        "        if len(forecast_np) != forecast_horizon:\n",
        "            if len(forecast_np) > forecast_horizon:\n",
        "                forecast_np = forecast_np[:forecast_horizon]\n",
        "            else:\n",
        "                # Pad with the last value if needed - use concatenation to avoid broadcasting issues\n",
        "                if len(forecast_np) > 0:\n",
        "                    last_val = float(forecast_np[-1])\n",
        "                    padding_length = forecast_horizon - len(forecast_np)\n",
        "                    padding = np.full(padding_length, last_val)\n",
        "                    forecast_np = np.concatenate([forecast_np, padding])\n",
        "                else:\n",
        "                    # If no valid forecast, fill with zeros\n",
        "                    forecast_np = np.zeros(forecast_horizon)\n",
        "        \n",
        "        return forecast_np\n",
        "\n",
        "print(\"ChronosForecast class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the benchmark path and model parameters. Adjust these according to your setup.\n",
        "ma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benchmark path: ../../multits-eval-nested/\n",
            "Model: amazon/chronos-bolt-base\n",
            "Device: cuda:0\n",
            "Output directory: ../../multieval_runs/chronos_bolt\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "BENCHMARK_PATH = \"../../multits-eval-nested/\"  # Adjust this path to your MultiTS-Eval data\n",
        "MODEL_PATH = \"amazon/chronos-bolt-base\"  # Chronos Bolt model\n",
        "DEVICE = \"cuda:0\"  # Use \"cpu\" if you don't have CUDA\n",
        "NUM_SAMPLES = 20  # Number of samples for probabilistic forecasting\n",
        "MAX_WINDOWS = 50  # Limit windows per dataset for faster testing\n",
        "OUTPUT_DIR = \"../../multieval_runs/chronos_bolt\"\n",
        "HISTORY_LENGTH = 512\n",
        "FORECAST_HORIZON = 128\n",
        "STRIDE = 256\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Benchmark path: {BENCHMARK_PATH}\")\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Chronos Model\n",
        "\n",
        "Create a ChronosForecast instance that integrates with the MultiTS-FM framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/caleb/MUSED-FM/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded Chronos model: amazon/chronos-bolt-base\n",
            "Chronos model initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize Chronos model\n",
        "try:\n",
        "    chronos_model = ChronosForecast(\n",
        "        model_path=MODEL_PATH,\n",
        "        device=DEVICE,\n",
        "        num_samples=NUM_SAMPLES,        \n",
        "    )\n",
        "    print(\"Chronos model initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Chronos model: {e}\")\n",
        "    print(\"Make sure you have installed chronos-forecasting and have the required dependencies.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Chronos Model Directly in Notebook\n",
        "\n",
        "Instead of using `run_multieval.py`, we can run Chronos directly in the notebook for more control and immediate results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading MultiTS-Eval benchmark...\n",
            "Loading KITTI data from ../../multits-eval-nested/sequential/KITTI\n",
            "Found 6114 parquet files\n",
            "Successfully loaded 6114 valid files\n",
            "Domain ALL_DATASETS not found in file hierarchy\n",
            "loading window counts from ../../multits-eval-nested/sequential_window_counts_h512_f128_s256.json\n",
            "loaded 8 cached window counts for category sequential\n",
            "['hopper_csv_out', 'spriteworld', 'ant_csv_out', 'cheetah_csv_out', 'walker2d_csv_out', 'KITTI', 'openwebtext_timeseries_csvs', 'cifar100_timeseries_csvs']\n",
            "successfully counted windows from cached JSON files\n",
            "Dataset aus_electricity not found in data_hierarchy.json\n",
            "Loading ECL data from ../../multits-eval-nested/traditional/ecl\n",
            "Found 1 parquet files\n",
            "Successfully loaded 206 valid chunks\n",
            "Dataset aus_electricity_nsw not found in file hierarchy\n",
            "Dataset aus_electricity_qld not found in file hierarchy\n",
            "Dataset cursor-tabs not found in file hierarchy\n",
            "Domain ALL_DATASETS not found in file hierarchy\n",
            "loading window counts from ../../multits-eval-nested/traditional_window_counts_h512_f128_s256.json\n",
            "loaded 67 cached window counts for category traditional\n",
            "['fl_electricity', 'ny_electricity2025', 'ecl', 'central_electricity', 'eastern_electricity', 'northern_electricity', 'southern_electricity', 'western_electricity', 'az_electricity', 'id_electricity', 'pa_electricity', 'tx_electricity', 'cal_electricity', 'ne_electricity', 'se_electricity', 'car_electricity', 'or_electricity', 'tn_electricity', 'al_daily', 'az_daily', 'cal_daily', 'car_daily', 'co_daily', 'ne_daily', 'nm_daily', 'ny_daily', 'pa_daily', 'tn_daily', 'tx_daily', 'ercot_load', 'solar_alabama', 'mds_microgrid', 'voip', 'ev-sensors', 'austin_water', 'mn_interstate', 'blue_bikes', 'traffic_PeMS', 'tac', 'paris_mobility', 'mta_ridership', 'lyft', 'uber', 'gas_sensor', 'oikolab_weather', 'open_aq', 'beijing_embassy', 'causalrivers', 'weather_mpi', 'beijing_aq', 'walmart-sales', 'gold_prices', 'bitcoin_price', 'rice_prices', 'pasta_sales', 'blow_molding', 'sleep_lab', 'cgm', 'fred_md1', 'fred_md2', 'fred_md3', 'fred_md4', 'fred_md5', 'fred_md6', 'fred_md7', 'fred_md8', 'website_visitors']\n",
            "successfully counted windows from cached JSON files\n",
            "Dataset simple_synthetic not a directory\n",
            "Domain ALL_DATASETS not found in file hierarchy\n",
            "loading window counts from ../../multits-eval-nested/synthetic_window_counts_h512_f128_s256.json\n",
            "loaded 6 cached window counts for category synthetic\n",
            "['large_convlag_synin_s', 'medium_convlag_synin_s', 'medium_obslag_synin_s', 'tiny_convlag_synin_ns', 'tiny_obslag_synin_ns', 'dynamic_data_csvs']\n",
            "successfully counted windows from cached JSON files\n",
            "Domain ALL_DATASETS not found in file hierarchy\n",
            "loading window counts from ../../multits-eval-nested/collections_window_counts_h512_f128_s256.json\n",
            "loaded 2 cached window counts for category collections\n",
            "['wikipedia', 'stock_nasdaqtrader']\n",
            "successfully counted windows from cached JSON files\n",
            "Benchmark loaded successfully!\n",
            "Number of categories: 4\n",
            "Category: sequential (4 domains)\n",
            "  Domain: Scientific (5 datasets)\n",
            "  Domain: Video (1 datasets)\n",
            "  Domain: Text (1 datasets)\n",
            "  Domain: Image (1 datasets)\n",
            "Category: traditional (8 domains)\n",
            "  Domain: Energy (32 datasets)\n",
            "  Domain: Engineering (2 datasets)\n",
            "  Domain: Public Info (9 datasets)\n",
            "  Domain: Environment (7 datasets)\n",
            "  Domain: Sales (6 datasets)\n",
            "  Domain: Health (2 datasets)\n",
            "  Domain: Finance (8 datasets)\n",
            "  Domain: Web (1 datasets)\n",
            "Category: synthetic (2 domains)\n",
            "  Domain: Causal Model (5 datasets)\n",
            "  Domain: Dynamic (1 datasets)\n",
            "Category: collections (2 domains)\n",
            "  Domain: Wikipedia (1 datasets)\n",
            "  Domain: Stock (1 datasets)\n",
            "Total datasets in benchmark: 83\n"
          ]
        }
      ],
      "source": [
        "# Load the MultiTS-Eval benchmark\n",
        "print(\"Loading MultiTS-Eval benchmark...\")\n",
        "try:\n",
        "    benchmark = Benchmark(BENCHMARK_PATH, history_length=HISTORY_LENGTH, forecast_horizon=FORECAST_HORIZON, stride=STRIDE, load_cached_counts=True)\n",
        "    print(f\"Benchmark loaded successfully!\")\n",
        "    print(f\"Number of categories: {len(benchmark)}\")\n",
        "    \n",
        "    # Print some basic info about the benchmark\n",
        "    total_datasets = 0\n",
        "    for category in benchmark:\n",
        "        print(f\"Category: {category.category} ({len(category)} domains)\")\n",
        "        for domain in category:\n",
        "            print(f\"  Domain: {domain.domain_name} ({len(domain)} datasets)\")\n",
        "            total_datasets += len(domain)\n",
        "    \n",
        "    print(f\"Total datasets in benchmark: {total_datasets}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading benchmark: {e}\")\n",
        "    print(f\"Make sure the benchmark path '{BENCHMARK_PATH}' is correct and contains MultiTS-Eval data.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Direct Evaluation\n",
        "\n",
        "Now let's run Chronos directly on the benchmark data for immediate results and full control.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Chronos evaluation...\n",
            "Starting direct evaluation on up to 3 datasets...\n",
            "\n",
            "Evaluating dataset: hopper_csv_out (1/3)\n",
            "Category: sequential, Domain: Scientific\n",
            "Dataset size: 33117 windows\n",
            "Processed 5 windows\n",
            "Average MAPE: 234.0065\n",
            "Average MAE: 6.4502\n",
            "Average RMSE: 7.9760\n",
            "Average NMAE: 1.0689\n",
            "\n",
            "Evaluating dataset: spriteworld (2/3)\n",
            "Category: sequential, Domain: Scientific\n",
            "Dataset size: 156272 windows\n",
            "Processed 5 windows\n",
            "Average MAPE: 106.8181\n",
            "Average MAE: 0.4516\n",
            "Average RMSE: 0.7205\n",
            "Average NMAE: 0.9895\n",
            "\n",
            "Evaluating dataset: ant_csv_out (3/3)\n",
            "Category: sequential, Domain: Scientific\n",
            "Dataset size: 19659 windows\n",
            "Processed 5 windows\n",
            "Average MAPE: 232.4465\n",
            "Average MAE: 0.4740\n",
            "Average RMSE: 1.2097\n",
            "Average NMAE: 0.4060\n",
            "\n",
            "Direct evaluation completed on 3 datasets\n"
          ]
        }
      ],
      "source": [
        "# Direct evaluation function\n",
        "def evaluate_chronos_directly(benchmark, model, max_datasets=5, max_windows_per_dataset=10):\n",
        "    \"\"\"\n",
        "    Directly evaluate Chronos model on benchmark data.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    dataset_count = 0\n",
        "    \n",
        "    print(f\"Starting direct evaluation on up to {max_datasets} datasets...\")\n",
        "    \n",
        "    for category in benchmark:\n",
        "        if dataset_count >= max_datasets:\n",
        "            break\n",
        "            \n",
        "        for domain in category:\n",
        "            if dataset_count >= max_datasets:\n",
        "                break\n",
        "                \n",
        "            for dataset in domain:\n",
        "                if dataset_count >= max_datasets:\n",
        "                    break\n",
        "                    \n",
        "                print(f\"\\nEvaluating dataset: {dataset.dataset_name} ({dataset_count + 1}/{max_datasets})\")\n",
        "                print(f\"Category: {category.category}, Domain: {domain.domain_name}\")\n",
        "                print(f\"Dataset size: {len(dataset)} windows\")\n",
        "                \n",
        "                # Limit windows for faster evaluation\n",
        "                windows_processed = 0\n",
        "                dataset_metrics = {'MAPE': [], 'MAE': [], 'RMSE': [], 'NMAE': []}\n",
        "                \n",
        "                for window in dataset:\n",
        "                    if windows_processed >= max_windows_per_dataset:\n",
        "                        break\n",
        "                        \n",
        "                    # Get history and future data\n",
        "                    history = window.history()\n",
        "                    target = window.target()\n",
        "                    covariates = window.covariates()\n",
        "                    forecast_horizon = len(target)\n",
        "                    \n",
        "                    # Generate forecast\n",
        "                    forecast = model.forecast(\n",
        "                        history=history,\n",
        "                        forecast_horizon=forecast_horizon,\n",
        "                        covariates=covariates\n",
        "                    )\n",
        "                    \n",
        "                    # Calculate metrics\n",
        "                    mape = MAPE(target, forecast)\n",
        "                    mae = MAE(target, forecast)\n",
        "                    rmse = RMSE(target, forecast)\n",
        "                    nmae = NMAE(target, forecast)\n",
        "                    \n",
        "                    dataset_metrics['MAPE'].append(mape)\n",
        "                    dataset_metrics['MAE'].append(mae)\n",
        "                    dataset_metrics['RMSE'].append(rmse)\n",
        "                    dataset_metrics['NMAE'].append(nmae)\n",
        "                    \n",
        "                    windows_processed += 1\n",
        "                        \n",
        "                \n",
        "                # Calculate average metrics for this dataset\n",
        "                if windows_processed > 0:\n",
        "                    avg_metrics = {}\n",
        "                    for metric_name, values in dataset_metrics.items():\n",
        "                        if values:\n",
        "                            avg_metrics[metric_name] = np.mean(values)\n",
        "                        else:\n",
        "                            avg_metrics[metric_name] = np.nan\n",
        "                    \n",
        "                    result = {\n",
        "                        'dataset': dataset.dataset_name,\n",
        "                        'category': category.category,\n",
        "                        'domain': domain.domain_name,\n",
        "                        'windows_processed': windows_processed,\n",
        "                        **avg_metrics\n",
        "                    }\n",
        "                    results.append(result)\n",
        "                    \n",
        "                    print(f\"Processed {windows_processed} windows\")\n",
        "                    print(f\"Average MAPE: {avg_metrics['MAPE']:.4f}\")\n",
        "                    print(f\"Average MAE: {avg_metrics['MAE']:.4f}\")\n",
        "                    print(f\"Average RMSE: {avg_metrics['RMSE']:.4f}\")\n",
        "                    print(f\"Average NMAE: {avg_metrics['NMAE']:.4f}\")\n",
        "                \n",
        "                dataset_count += 1\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run direct evaluation\n",
        "print(\"Starting Chronos evaluation...\")\n",
        "direct_results = evaluate_chronos_directly(benchmark, chronos_model, max_datasets=3, max_windows_per_dataset=5)\n",
        "print(f\"\\nDirect evaluation completed on {len(direct_results)} datasets\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
